{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Backward-Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps taken:\n",
    "1. Preprocess features with non-linear transformations\n",
    "2. Tune SVC using IC50<10 threshold for classes (need to try new values)\n",
    "3. Use tuned model for backward selection, par operations for speed\n",
    "    * Set benchmark using log loss or roc with prob output from SVC\n",
    "    * Use CV=sum(y_class) for all loss calculations\n",
    "    * If removal of feature improved loss, add to removals list\n",
    "    * Remove up to 10% of features available starting with lowest loss\n",
    "4. Analyze output of new features with confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skewtest\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from custom_functions import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "from par_support import par_backward_stepwise\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail_transformations = [\"log\", \"log2\", \"log10\", \"cubert\", \n",
    "                         \"sqrt\", \"exp\", \"exp2\", \"cube\", \"sq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in full dataset\n",
    "full_x, full_y = load_full_dataset()\n",
    "# Preprocess variables\n",
    "full_x = preprocess_variables(full_x)\n",
    "# Extract list of available columns\n",
    "full_columns = full_x.columns\n",
    "print(\"Loading in compound dataset....\")\n",
    "# Read in compound dataset\n",
    "compound_x, compound_y = load_compound_dataset()\n",
    "# Preprocess\n",
    "compound_x = preprocess_variables(compound_x)\n",
    "# Find intersecting features\n",
    "avail_columns = compound_x.columns.intersection(full_columns)\n",
    "# Select features on subset\n",
    "x_data = compound_x.loc[:, avail_columns]\n",
    "y_data = compound_y.copy()\n",
    "print(\"Adding non-linear features to compound dataset....\")\n",
    "# Add all transformations on compound data\n",
    "for feature in x_data.columns[x_data.dtypes == 'float64']:\n",
    "    x_data = add_transformations(x_data, feature)\n",
    "# Drop any new columns with NaN due to improper transformation\n",
    "x_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "x_data.dropna(axis=1, inplace=True)\n",
    "assert not sum(x_data.isna().sum()), \"Unexpected nulls found\"\n",
    "y_class = np.squeeze([int(y_val <= 10) for y_val in y_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection using: SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score using log_loss and roc (compare results)\n",
    "scaler = StandardScaler()\n",
    "# Scale the train data\n",
    "x_train = scaler.fit_transform(x_data)\n",
    "x_train = pd.DataFrame(data=x_train, columns=x_data.columns, index=x_data.index)\n",
    "\n",
    "# # Set params for tuning\n",
    "model = SVC(random_state=0, probability=True)\n",
    "params = {\"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "          \"C\": np.arange(0.05, 1.05, .05),\n",
    "          \"class_weight\": [None, \"balanced\"]}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid=params, scoring=make_scorer(roc_auc_score),\n",
    "                    cv=sum(y_class), n_jobs=7)\n",
    "grid.fit(x_train, y_class)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(random_state=0, class_weight=\"balanced\", kernel=\"sigmoid\", probability=True, C=0.95)\n",
    "\n",
    "benchmark = np.mean(cross_val_score(model, x_train, y_class,\n",
    "                                    scoring=make_scorer(roc_auc_score),\n",
    "                                    cv=sum(y_class), n_jobs=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use parallel loop to calculate each features removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_in = list(x_train.columns)\n",
    "\n",
    "while True:  # While features to remove, add break\n",
    "    higher_is_better = True\n",
    "    start = time()\n",
    "    results_par = Parallel(n_jobs=7)(\n",
    "        delayed(par_backward_stepwise)(features, x_train, y_class, model) for features in np.array_split(features_in, 7))\n",
    "    norm_summary = [item for sublist in results_par for item in sublist]\n",
    "    stop = time()\n",
    "    print((stop - start))\n",
    "\n",
    "    # TODO update benchmark\n",
    "    return_dict = dict()\n",
    "    for d in results_par:\n",
    "        # assert isinstance(d, dict)\n",
    "        for k, v in d.items():\n",
    "            return_dict[k] = v\n",
    "\n",
    "    # If list len is 0 then stop, no more features to remove\n",
    "    # Intuition: greater than benchmark means removal increased the loss function\n",
    "    if higher_is_better:\n",
    "        potential_removals = {feat: return_dict[feat] \n",
    "                              for feat, roc in return_dict.items() \n",
    "                              if roc >= benchmark}\n",
    "    else:\n",
    "        potential_removals = {feat: return_dict[feat] \n",
    "                              for feat, roc in return_dict.items() \n",
    "                              if roc <= benchmark}\n",
    "    if len(potential_removals) == 0:\n",
    "        print(\"nothing to remove\")\n",
    "        break\n",
    "\n",
    "    # Remove features with the best scores (top 10%) intuition: removing them led to improved scores\n",
    "    filter_to = max(len(return_dict) - len(potential_removals), int(len(return_dict) * .90))\n",
    "    features_in = sorted(return_dict.keys(), key=lambda x: return_dict[x], reverse=higher_is_better)[:filter_to]\n",
    "\n",
    "    # Set benchmark\n",
    "    benchmark = np.mean(cross_val_score(model, x_train[features_in], y_class,\n",
    "                                        scoring=make_scorer(roc_auc_score),\n",
    "                                        cv=sum(y_class), n_jobs=7))\n",
    "print(\"final score: %s\" % str(benchmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV prediction performance\n",
    "predict = cross_val_predict(\n",
    "    model, x_data[selected_features], y_class, cv=sum(y_class), method=\"predict\")\n",
    "\n",
    "print(confusion_matrix(y_class, predict, labels=[1, 0]))\n",
    "print(np.array([[\"TP\", \"FN\"], [\"FP\", \"TN\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try with adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learn]",
   "language": "python",
   "name": "conda-env-deep-learn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

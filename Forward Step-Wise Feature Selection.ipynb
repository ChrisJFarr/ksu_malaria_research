{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection: Forward Step-Wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far I've tried lasso regression on all datapoints to select features including non-linearly transformed features. A polynomial PCA of 21 selected features shows a pattern in the potent compounds, but not good separation from inactive compounds.\n",
    "\n",
    "I want to explore new methods for feature selection and compare these methods using the predictive accuracy of OSM-S-106 potency and through visualizations of their dimensionality. To visually show the performance of the models, select models that output feature importance and pass through all principal components of the underlying features, then use the most important dimensions in the visuals.\n",
    "\n",
    "**Methods to combine**\n",
    "* Forward step-wise feature selection\n",
    "* Include decoy data-points\n",
    "* Cross-Validate with held out potent compounds for step-wise selection\n",
    "* Test feature selection and models with OSM-S-106 for final comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: \n",
    "* A model can accurately predict the potency of OSM-S-106 without using it in the training data or any derivative from it's data\n",
    "\n",
    "Assumption:\n",
    "* At least some of the relatively potent compounds measured must perform in a similar way to OSM-S-106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import skewtest\n",
    "from sklearn.linear_model import Lasso\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data and pre-process variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Series3_6.15.17_padel.csv\")\n",
    "# Drop examples without IC50\n",
    "df = df[~df.IC50.isnull()]\n",
    "\n",
    "# Column types and counts\n",
    "np.unique(df.dtypes)\n",
    "len(df.columns[df.iloc[:, :].dtypes == 'O'])\n",
    "len(df.columns[df.iloc[:, :].dtypes == 'int64'])\n",
    "len(df.columns[df.iloc[:, :].dtypes == 'float64'])\n",
    "\n",
    "\"\"\" Preprocessing Variables \"\"\"\n",
    "# Categorical Variables: No missing values\n",
    "sum(df[df.columns[df.iloc[:, :].dtypes == 'int64']].isnull().sum())\n",
    "# Get dummy vars: filter to int type, convert to object, pass to get_dummies.\n",
    "cat_vars_df = pd.get_dummies(\n",
    "    df[df.columns[df.iloc[:, :].dtypes == 'int64']].astype('O'))\n",
    "\n",
    "# Continuous Variables: 67 columns have missing values\n",
    "sum(df[df.columns[df.iloc[:, :].dtypes == 'float64']].isnull().sum())\n",
    "# Impute or remove? (for now remove any columns with nan)\n",
    "cont_vars_df = df[df.columns[df.iloc[:, :].dtypes == 'float64']].dropna(axis=1)\n",
    "# Drop target variable\n",
    "cont_vars_df.drop(\"IC50\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process continuous variables: Add transformations, remove skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transformations(df, feat):\n",
    "    feature_df = df.loc[:, feat].copy()\n",
    "    if feature_df.min() > 0:  # Avoid 0 or negative\n",
    "        df.loc[:, feat + \"_log\"] = feature_df.apply(np.log)  # log\n",
    "        df.loc[:, feat + \"_log2\"] = feature_df.apply(np.log2)  # log2\n",
    "        df.loc[:, feat + \"_log10\"] = feature_df.apply(np.log10)  # log10\n",
    "    df.loc[:, feat + \"_cubert\"] = feature_df.apply(\n",
    "        lambda x: np.power(x, 1/3))  # cube root\n",
    "    df.loc[:, feat + \"_sqrt\"] = feature_df.apply(np.sqrt)  # square root\n",
    "    if feature_df.max() < 10:  # Avoid extremely large values\n",
    "        df.loc[:, feat + \"_sq\"] = feature_df.apply(np.square)  # square\n",
    "        df.loc[:, feat + \"_cube\"] = feature_df.apply(\n",
    "            lambda x: np.power(x, 3))  # cube\n",
    "        df.loc[:, feat + \"_exp\"] = feature_df.apply(np.exp)  # exp\n",
    "        df.loc[:, feat + \"_exp2\"] = feature_df.apply(np.exp2)  # exp2\n",
    "    return df\n",
    "\n",
    "# Add above transformations for all continuous variables\n",
    "for feature in cont_vars_df.columns:\n",
    "    cont_vars_df = add_transformations(cont_vars_df, feature)\n",
    "# Drop any new columns with NaN due to improper transformation\n",
    "cont_vars_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "cont_vars_df.dropna(axis=1, inplace=True)\n",
    "\n",
    "# Assume skewed if we can reject the null hypothesis with 95% certainty\n",
    "# Remove any skewed features after adding transformations\n",
    "cont_vars_df = cont_vars_df.loc[:, cont_vars_df.apply(\n",
    "    lambda x: skewtest(x)[1] > .05).values]\n",
    "\n",
    "# Combine datasets\n",
    "vars_df = pd.concat([cat_vars_df, cont_vars_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learn]",
   "language": "python",
   "name": "conda-env-deep-learn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Backward-Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps taken:\n",
    "1. Preprocess features with non-linear transformations\n",
    "2. Tune SVC using IC50<10 threshold for classes (need to try new values)\n",
    "3. Use tuned model for backward selection, par operations for speed\n",
    "    * Set benchmark using log loss or roc with prob output from model\n",
    "    * Use CV=sum(y_class) for all performance calculations\n",
    "    * If removal of feature improved model performance, add to removals list\n",
    "    * Group features for removals by correlation groups, only select the worst from each group each iteration\n",
    "4. Analyze output of new features with confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skewtest\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from custom_functions import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "from par_support import par_backward_stepwise\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail_transformations = [\"log\", \"log2\", \"log10\", \"cubert\", \n",
    "                         \"sqrt\", \"exp\", \"exp2\", \"cube\", \"sq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Akt1_decoys_padel.csv....\n",
      "Adding AmpC_decoys_padel.csv....\n",
      "Adding cp3a4_decoys_padel.csv....\n",
      "Adding cxcr4_decoys_padel.csv....\n",
      "Adding gcr_decoys_padel.csv....\n",
      "Adding HIVpr_decoys_padel.csv....\n",
      "Adding HIVrt_decoys_padel.csv....\n",
      "Adding Kif11_decoys_padel.csv....\n",
      "Loading in compound dataset....\n",
      "Adding non-linear features to compound dataset....\n"
     ]
    }
   ],
   "source": [
    "# Load in full dataset\n",
    "full_x, full_y = load_full_dataset()\n",
    "# Preprocess variables\n",
    "full_x = preprocess_variables(full_x)\n",
    "# Extract list of available columns\n",
    "full_columns = full_x.columns\n",
    "print(\"Loading in compound dataset....\")\n",
    "# Read in compound dataset\n",
    "compound_x, compound_y = load_compound_dataset()\n",
    "# Preprocess\n",
    "compound_x = preprocess_variables(compound_x)\n",
    "# Find intersecting features\n",
    "avail_columns = compound_x.columns.intersection(full_columns)\n",
    "# Select features on subset\n",
    "x_data = compound_x.loc[:, avail_columns]\n",
    "y_data = compound_y.copy()\n",
    "print(\"Adding non-linear features to compound dataset....\")\n",
    "# Add all transformations on compound data\n",
    "for feature in x_data.columns[x_data.dtypes == 'float64']:\n",
    "    x_data = add_transformations(x_data, feature)\n",
    "# Drop any new columns with NaN due to improper transformation\n",
    "x_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "x_data.dropna(axis=1, inplace=True)\n",
    "assert not sum(x_data.isna().sum()), \"Unexpected nulls found\"\n",
    "# Create binary variable\n",
    "y_class = np.squeeze([int(y_val <= 10) for y_val in y_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier(random_state=0, learning_rate=0.075, n_estimators=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  4]\n",
      " [ 4 32]]\n",
      "[['TP' 'FN']\n",
      " ['FP' 'TN']]\n"
     ]
    }
   ],
   "source": [
    "# Analyze CV prediction performance\n",
    "predict = cross_val_predict(\n",
    "    model, x_data, y_class, cv=sum(y_class), method=\"predict\")\n",
    "\n",
    "predict_proba = cross_val_predict(\n",
    "    model, x_data, y_class, cv=sum(y_class), method=\"predict_proba\")\n",
    "\n",
    "print(confusion_matrix(y_class, predict, labels=[1, 0]))\n",
    "print(np.array([[\"TP\", \"FN\"], [\"FP\", \"TN\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = roc_auc_score(y_class, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use parallel loop to calculate each features removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_function = make_scorer(roc_auc_score)\n",
    "higher_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark: 0.7121212121212122\n",
      "iteration time: 1173.6409928798676\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7462121212121212\n",
      "keep: 0.7462121212121212\n",
      "remove: 0.7575757575757577\n",
      "keep: 0.7575757575757577\n",
      "120 features removed\n",
      "new benchmark: 0.7575757575757577\n",
      "iteration time: 1139.2440869808197\n",
      "nothing to remove\n",
      "final score: 0.7575757575757577\n"
     ]
    }
   ],
   "source": [
    "features_in = list(x_train.columns)\n",
    "n_jobs = 4\n",
    "print(\"Starting benchmark: %s\" % str(benchmark))\n",
    "while True:  # While features to remove, add break\n",
    "    start = time()\n",
    "    results_par = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(par_backward_stepwise)(features, x_train[features_in], y_class, model, scoring_function) \n",
    "        for features in np.array_split(features_in, n_jobs))\n",
    "    stop = time()\n",
    "    print(\"iteration time: %s\" % str(stop - start))\n",
    "\n",
    "    # TODO update benchmark\n",
    "    return_dict = dict()\n",
    "    for d in results_par:\n",
    "        # assert isinstance(d, dict)\n",
    "        for k, v in d.items():\n",
    "            return_dict[k] = v\n",
    "\n",
    "    # If list len is 0 then stop, no more features to remove\n",
    "    # Intuition: greater than benchmark means removal increased the loss function\n",
    "    if higher_is_better:\n",
    "        potential_removals = {feat: return_dict[feat] \n",
    "                              for feat, score in return_dict.items() \n",
    "                              if score > benchmark}\n",
    "    else:\n",
    "        potential_removals = {feat: return_dict[feat] \n",
    "                              for feat, score in return_dict.items() \n",
    "                              if score < benchmark}\n",
    "    if len(potential_removals) == 0:\n",
    "        print(\"nothing to remove\")\n",
    "        break\n",
    "    \n",
    "    # Determine correlated groupings, only remove one from each group if correlated\n",
    "    df=x_train[list(potential_removals.keys())]\n",
    "    corr_matrix=df.corr()\n",
    "    corr_matrix.loc[:,:] =  np.tril(corr_matrix, k=-1) # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    corr_result = []\n",
    "    for col in corr_matrix:\n",
    "        correlated = corr_matrix[col][np.abs(corr_matrix[col]) > .7].index.tolist()\n",
    "        if correlated and col not in already_in:\n",
    "            already_in.update(set(correlated))\n",
    "            correlated.append(col)\n",
    "            corr_result.append(correlated)\n",
    "        elif col not in already_in:\n",
    "            already_in.update(set([col]))\n",
    "            corr_result.append([col])\n",
    "            \n",
    "    # For each set of correlated features...\n",
    "    # Sort by score from return_dict and remove only the highest score\n",
    "    final_removal = []\n",
    "    for corr_list in corr_result:\n",
    "        score_sorted = sorted(corr_list, key=lambda x: return_dict[x], reverse=not higher_is_better)\n",
    "        print(\"remove: %s\" % return_dict[score_sorted[-1]])\n",
    "        print(\"keep: %s\" % return_dict[score_sorted[0]])\n",
    "        final_removal.append(score_sorted[-1])\n",
    "\n",
    "    # Remove the worst feature from each correlated group\n",
    "    features_in = list(set(features_in) - set(final_removal))\n",
    "    print(\"%s features removed\" % str(len(final_removal)))\n",
    "\n",
    "    # Set benchmark\n",
    "    benchmark = np.mean(cross_val_score(model, x_train[features_in], y_class,\n",
    "                                        scoring=scoring_function,\n",
    "                                        cv=sum(y_class), n_jobs=n_jobs))\n",
    "    print(\"new benchmark: %s\" % str(benchmark))\n",
    "print(\"final score: %s\" % str(benchmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = features_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-9305860897cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     model, x_data[selected_features], y_class, cv=sum(y_class), method=\"predict_proba\")\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TP\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"FP\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\deep\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \"\"\"\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\deep\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "# Analyze CV prediction performance\n",
    "predict = cross_val_predict(\n",
    "    model, x_data[selected_features], y_class, cv=sum(y_class), method=\"predict\")\n",
    "\n",
    "print(confusion_matrix(y_class, predict, labels=[1, 0]))\n",
    "print(np.array([[\"TP\", \"FN\"], [\"FP\", \"TN\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try with adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
